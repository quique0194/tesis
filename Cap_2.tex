\chapter{Marco teórico: Aprendizaje por refuerzo en sistemas multiagente}

Le permite a un agente escoger acciones intereactuando con el ambiente. Modela el mundo mediante un \ac{MDP}.


\section{Modelo básico}

Consiste en las siguientes partes:
\begin{itemize}
    \item Conjunto de estados $S$. Un estado es un conjunto de características que indican como está el ambiente.
    \item Conjunto de acciones $A$. 
    \item Reglas de transición entre los estados
    \item Reglas que determinar la recompensa inmediata de una transición
    \item Reglas que describen lo que observa el agente
\end{itemize}


\subsection{Ecuación de bellman}

Ecuación de bellman


\subsubsection{Método de Montecarlo}

Método de Montecarlo


\subsubsection{Método de diferencias temporales}

Método de diferencias temporales


\subsection{Q-learning}

Q-learning


\subsection{Exploración vs Explotación}

Exploración vs Explotación


\subsection{Sistemas multiagente}

Definicion
Tipos
Equilibrio de Nash


\subsection{Aprendizaje en sistemas multiagente}

Aprendizaje como un equipo
Aprendizaje independiente
Aprendizaje de acciones conjuntas
Aprendizaje con valores de influencia


\section{Consideraciones Finales}

Cada capítulo excepto el primero debe contener al finalizarlo una sección de consideraciones que enlacen
el presente capítulo con el siguiente.
