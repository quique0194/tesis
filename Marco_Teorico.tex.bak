\chapter{Marco teórico}

\section{Reinforcement Learning}

% Breve intro a lo que es aprendizaje por refuerzo
El aprendizaje por refuerzo consiste en mapear situaciones a acciones, de forma que se maximice una recompensa numérica. Al agente no se le dice que acciones tomar como en muchas formas de aprendizaje de máquina, sino que debe descubrir que acciones brindan la mayor recompensa intentándolas. Es los casos más interesantes y desafiantes, las acciones pueden no sólo afectar la recompensa inmediata, sino la siguiente situación y en consecuencia, todas las recompensas siguientes. Estas dos características: búsqueda por prueba y error y recompensa retardada, son las dos características más importantes del aprendizaje por refuerzo.

% Elementos
Más allá del agente y el ambiente, se pueden identificar cuatro sub-elementos de un sistema de aprendizaje por refuerzo: la política, la función de recompensa, la función de valor y opcionalmente, un modelo del ambiente.

La \textit{política} define la forma de comportarse del agente en un tiempo dado. En otras palabras, la política es el mapeamiento de un estado percibido a una acción que debe ser tomada en ese estado. Corresponde a lo que en psicología sería llamado un conjunto de reglas de estímulo-respuesta. En algunos casos la política puede ser una función simple o una tabla, mientras que en otros puede invocar computación extensiva como un proceso de búsqueda. La política es el corazón del agente de aprendizaje por refuerzo en el sentido de que por si mismo puede determinar el comportamiento. En general, las políticas suelen ser estocásticas.

Una \textit{función de recompensa} define la meta en un problema de aprendizaje por refuerzo. Dicho de otro modo, mapea cada estado percibido (o par de estado-acción) a un número singular, la \textit{recompensa}, indicando el valor intrínseco de dicho estado. El único propósito de un agente de aprendizaje por refuerzo es maximizar la recompensa total que recibe en el largo plazo. En un sistema biológico, no sería inapropiado identificar las recompensas con placer y dolor. De esa forma, la función de recompensa debe ser necesariamente inalterable por el agente. Sin embargo, debe servir como una base para alterar la política. En general, las funciones de recompensa también son estocásticas.

Mientras que la función de recompensa indica que es bueno en sentido inmediato, una \textit{función de valor} especifica que es bueno en el largo plazo. En otras palabras, el valor de un estado es el monto total de recompensa que un agente puede esperar acumular en el futuro, empezando en ese estado. Para hacer una analogía más humana, las recompensas son como el placer o el dolor mientras que los valores corresponden a un juicio más definido y con visión del futuro.

El último elemento de algunos sistemas de aprendizaje por refuerzo es un \textit{modelo} del ambiente. Esto es algo que imita el comportamiento del ambiente. Por ejemplo, dado un estado y una acción, el modelo puede predecir el estado resultante y la siguiente recompensa. Los modelos son usados para planear, y por eso queremos decir cualquier forma de decidir un curso de acción considerando posibles situaciones futuras sin haberlas experimentado realmente.

\subsection{El problema del aprendizaje por refuerzo}

% Problema de RL, diagrama agente-ambiente
En esta sección hablamos del problema que trata de solucionar el aprendizaje por refuerzo. El problema del aprendizaje por refuerzo esta pensado como un marco de referencia sencillo para el problema de aprender a través de la interacción para lograr una meta.

% Definición formal de elementos
El agente y el ambiente interactúan en una secuencia de pasos discretos, $t = 0, 1, 2, 3, ...$. En cada paso $t$, el agente recibe una representación del estado del ambiente $S_t \in S$ donde $S$ es el conjunto de posibles estados, y en esa base selecciona una acción $A_t \in A(S_t)$, donde $A(S_t)$ es el conjunto de acciones disponibles en el estado $S_t$. Un paso de tiempo después, en parte como consecuencia de su acción, el agente recibe una recompensa numérica $R_{t+1} \in R$ y se encuentra a sí mismo en un nuevo estado $S_{t+1}$. La Figura %TODO1.
muestra la interacción del agente con el ambiente.

En cada paso de tiempo, el agente implementa un mapeo de estados a probabilidades de seleccionar cada posible acción. Este mapeamiento es llamado la política y se denota como $\pi_t$, donde $\pi_t(a|s)$ es la probabilidad de que $A_t=a$ si $S_t=s$. Los métodos de aprendizaje por refuerzo especifican como el agente cambia su política como resultado de la experiencia. La meta del agente es maximizar el monto total de recompensa que recibe en el largo plazo.

% Meta y recompensas + Retorno descontado
La meta y las recompensas están íntimamente ligadas. El propósito o meta del agente esta formalizado en términos de una señal de recompensa que el ambiente le pasa al agente. En cada paso de tiempo, la recompensa es un un simple número, $R_t \in R$. Informalmente, la meta del agente es maximizar el total de recompensa que recibe. Esto no significa maximizar la recompensa inmediata, sino la recompensa acumulada en el largo plazo. Pero ¿cómo se define esto formalmente? Si la secuencia de recompensas recibidas después del paso $t$ se denotan como $R_{t+1}, R_{t+2}, R_{t+3}, ...$, entonces ¿qué aspecto de esta secuencia queremos maximizar? En general, buscamos maximizar la recompensa acumulada, donde el retorno $G_t$, esta definido como una función de la secuencia de la recompensa. En el caso más simple, el retorno está definido como la suma de las recompensas:

\begin{equation} \label{eq:retorno_esperado_suma}
G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T
\end{equation}


Este enfoque tiene sentido en aplicaciones donde hay un paso final, y las interacciones del agente se dividen en subsecuencias que llamaremos \textit{episodios}. Sin embargo, existen casos donde las interacciones del agente continúan sin un límite, a estas tareas las llamaremos \textit{tareas continuas}. La fórmula del retorno (\ref{eq:retorno_esperado_suma}) es problemática para tareas continuas, ya que el retorno podría ser infinito. Para solucionar esto necesitamos un concepto adicional, el \textit{descuento}. De acuerdo a este enfoque, el agente trata de seleccionar acciones de modo que la suma descontada de las recompensas que recibe en el futuro sea maximizada. La fórmula del \textit{retorno descontado} es:


\begin{equation} \label{eq:retorno_esperado_descuento}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... 
\end{equation}

Donde $\gamma$ es un parámetro, $0 \leq \gamma \leq 1$, llamado el \textit{factor de descuento}.

Para unificar la visión de las tareas episódicas y continuas, podemos considerar que el final de un episodio sea la entrada a un \textit{estado de absorción} que siempre dirige al agente al mismo estado y retorna sólo recompensas de cero. Como en el diagrama %TODO2.

% Propiedad de Markov
Las señales que provienen del ambiente deben cumplir una propiedad importante: La propiedad de Markov. Esta propiedad indica que la señal de estado debe incluir las sensaciones pasadas de forma compacta, de forma que toda la información relevante es retenida. Una tarea de aprendizaje por refuerzo que satisface la propiedad de Markov se llama \textit{proceso de decisión de Markov}, o \textit{MDP}. 

% Función de valor y función de valor óptima
Casi todos los algoritmos de aprendizaje por refuerzo involucran estimar una \textit{función de valor}, que en palabras sencillas significa estimar que tan buena es el estado donde se encuentra el agente, en base a la recompensas futuras que pueden ser esperadas. Pero por supuesto que las recompensas que el agente puede esperar recibir dependen de que acciones va a tomar. De este modo y es la recompensa esperada empezando en el estado $s$ y siguiendo la política $\pi$. De forma similar, denotamos el valor de tomar la acción $a$ en el estado $s$ bajo una política $\pi$ como $q_\pi(s,a)$.

Una propiedad fundamental de las funciones de valor es la \textit{ecuación de Bellman} %TODO3.
Para cualquier política $\pi$ y cualquier estado $s$, la siguiente condición se mantiene entre el valor de $s$ y el valor de sus posibles sucesores:

\begin{equation} \label{eq:bellman}
v_\pi(s) = \sum_a \pi(a|s) \sum_{s'} p(s'|s,a) \left[r(s,a,s') + \gamma E_\pi \left[\sum_{k=0}^{\infty} R_{t+k+2} | S_{t+1}=s'\right]\right]
\end{equation}

Resolver una tarea de aprendizaje por refuerzo significa, brevemente, encontrar una política que consiga mucha recompensa en el largo plazo. Para MDPs finitos, podemos definir una política óptima de la siguiente manera. Una política $\pi$ es mejor o igual que otra política $\pi'$ si su recompensa esperada es mayor o igual que la de $\pi'$ para todos los estados. Siempre hay al menos una política que es mejor o igual que todas las demás. Esta es una política óptima, que aunque pueden ser más de una, las denotamos como $\pi_*$.

% Optimalidad y aproximación
Hemos definido funciones de valor óptimas y políticas óptimas. Sin embargo, lograr que un agente aprenda estas cosas en la práctica rara vez sucede. Para el tipo de tareas en que estamos interesados, las políticas óptimas sólo pueden ser generadas con un costo computacional extremo. El enfoque que se toma es usar aproximaciones, por ejemplo, hay mucho estados que el agente enfrentará con muy poca probabilidad, así que tomar acciones subóptimas en estos estados no es muy relevante en las recompensas que el agente recibe. La naturaleza online del aprendizaje por refuerzo hace que sea posible aproximar políticas óptimas en formas que ponen más esfuerzo en aprender a tomar buenas decisiones para estados encontrados frecuentemente. Esta es una propiedad clave que distingue el aprendizaje por refuerzo de otros enfoques para resolver aproximadamente MDPs.

% Exploración vs explotación
Siempre existe el problema de balancear la exploración con la explotación. Siendo que la explotación nos permite utilizar los conocimientos que ya tenemos para obtener la máxima recompensa, pero la exploración nos permite obtener un mejor conocimiento y así mejorar nuestras decisiones. Si sólo se hiciera exploración, no tendría sentido puesto que nunca se explotaría el conocimiento generado. Y si siempre se hiciera explotación, quedaríamos atorados en la toma de decisiones sub-óptimas. Lo ideal es que haya una alta tasa de exploración al comienzo y que esta vaya disminuyendo hasta un valor pequeño estable con el tiempo.


\subsection{Iteración Generalizada de Política (GPI)}

La iteración de política consiste en dos procesos simultáneos que interactúan. Uno hace que la función de valor sea consistente con la política actual (evaluación de política), y el otro hace a la política avara respecto a la función de valor actual (mejora de política). En la iteración de política, estos procesos se alternan, completándose uno antes de que el otro comience, pero esto no es realmente necesario.

Llamamos \ac{GPI} a la idea de dejar que ambos procesos, la evaluación de la política y la mejora de la política, interactúen, independientemente de la granularidad y otros detalles de los dos procesos \cite{van1978discounted}. El esquema general de \ac{GPI} esta ilustrado en la figura \ref{fig:gpi}.

\begin{figure}[htb]
\centering
\includegraphics[width=40mm]{./graficos/gpi.png}
\caption{GPI} \label{fig:gpi}
\end{figure}


\subsection{On-policy vs Off-policy}

Los métodos on-policy tratan de evaluar y mejorar la política que está siendo usada para tomar decisiones. Imagine en cambio que queremos mejorar una determinada política $\pi$ estimando $v_\pi$ o $q_\pi$; pero sólo disponemos de episodios generados por otra política $\mu \neq \pi$. Llamamos a $\pi$ la política objetivo, porque aprender su función de valor es el objetivo del proceso de aprendizaje, mientras que a $\mu$ la llamamos política de comportamiento porque es la política que controla al agente y genera el comportamiento \cite{precup2001off}.


\subsection{Método actor-critic}

Consiste en tener una estructura de memoria separada para representar explícitamente la política independientemente de la función de valor. La estructura de la política es conocida como el actor, porque se usa para seleccionar acciones, y la función estimada de valor es conocida como el crítico, porque critica las acciones hechas por el actor \cite{barto2004j}.

Las críticas toman la forma del error, por ejemplo, en el caso de diferencias temporales, la crítica sería:

\begin{equation} \label{eq:critica_td}
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\end{equation}

Donde $V$ es la función de valor actual implementada por el crítico. Este error puede ser usado para evaluar la acción recién tomada $A_t$. Si el error es positivo, la tendencia a seleccionar $A_t$ debería incrementar, pero si el error es negativo, dicha tendencia debería disminuir.


\subsection{Políticas parametrizadas}

Cuando el número de estados es demasiado grande, ya no es conveniente mantener promedios separados para cada uno en una tabla. En su lugar, el agente puede mantener la función de valor $v_\pi$ o $q_\pi$ como funciones parametrizadas y ajustar dichos parámetros para ajustarse mejor a los retornos observados. Esto también puede producir estimaciones precisas, aunque depende mucho del aproximador de función parametrizado que se escoja.


\subsection{Métodos de Gradiente de Política}

Son métodos de \ac{RL} que optimizan políticas parametrizadas respecto al retorno esperado usando la gradiente descendente \cite{sutton1999policy}.

Se asume que podemos modelar el sistema en forma de tiempo discreto y denotaremos el tiempo presente como $t$. Para tener en cuenta el factor estocástico del modelo, denotamos el cambio de estados usando una distribución de probabilidad $s_{t+1} \sim p(s_{t+1}|s_t, a_t)$ como modelo donde $a_t \in \mathbb{R}^N$ denota la acción actual, y $s_t, s_{t+1} \in \mathbb{R}^N$ denotan los estados actual y siguiente. También se asume que las acciones son generadas por una política $\pi(a_t|s_t)$ que se modela como una función de probabilidad para incorporar acciones exploratorias. Se asume que esta política está parametrizada por $k$ parámetros $\theta \in \mathbb{R}^k$. La secuencia de estados y acciones forman una trayectoria denotada por $\tau = [s_{0:H}, a_{0:H}]$ donde $H$ denota el horizonte que puede ser infinito. En cada instante de tiempo, el sistema recibe una recompensa denotada $r_t = r(s_t, a_t) \in \mathbb{R}$.

El objetivo general de la optimización de política en \ac{RL} es optimizar los parámetros de la política $\theta \in \mathbb{R}^k$ de forma que el retorno esperado se optimize.

\begin{equation}
J(\theta) = \left\lbrace \sum_{t=0}^H \gamma^t r_t \right\rbrace
\end{equation}

Para eso, los métodos de gradiente de política seguirán la gradiente ascendente del retorno esperado para actualizar los parámetros $\theta \in \mathbb{R}^k$.

\begin{equation}
\theta_{h+1} = \theta_h + \alpha_h \nabla_\theta J_{\theta = \theta_h}
\end{equation}

Donde $\alpha_h \in \mathbb{R}^+$ es la tasa de aprendizaje y $h = \{0,1,2,...\}$ es el número de la actualización actual. Nótese que el tiempo $t$ y el número de actualización $h$ son diferentes, por ejemplo, imagine un aprendizaje episódico donde la actualización se realiza sólo al final de cada episodio.



\section{Redes Neuronales y Deep Learning}

\subsection{Multilayer perceptron}

El \ac{MLP} es una red neuronal formada por mútliples capas que le permiten aproximar funciones no lineales \cite{yang2010multi}. La arquitectura del perceptron multicapa la podemos observar en la figura \ref{fig:mlp}.

\begin{figure}[htb]
\centering
\includegraphics[width=150mm]{./graficos/mlp.jpg}
\caption{MLP} \label{fig:mlp}
\end{figure}

Básicamente tenemos entradas $x_i$ y salidas $y_i$. La red neuronal es una función paramétrica con parámetros $w$ que aproxima $y = f(x)$ modificando sus parámetros. Podemos ver a la red neuronal como una función $g(x, w) \sim f(x)$. El algoritmo que usa para aproximarse más a $f(x)$ es el de la gradiente descendente del error respecto a sus parámetros. El error puede ser cualquier función de error entre $f(x)$ y $g(x, w)$, siendo la más común la función de error cuadrático.

\begin{equation}
w = w - \alpha \nabla_w E
\end{equation}

\subsection{Restricted Boltzman Machine}

Las \ac{RBM} son redes neuronales generativas estocásticas que pueden aprender una distribución de probabilidad sobre su conjunto de entradas \cite{hinton2010practical}. La arquitectura de una \ac{RBM} se encuentra en la figura \ref{fig:rbm}. Consiste de nodos de unidades visibles $x$ y unidades escondidas $h$. Cada unidad visible está conectada a todas las unidades escondidas y viceversa. Cada conexión tiene un peso $w$ asociado, que es el parámetro que se busca optimizar mediante el entrenamiento. Cada unidad visible e invisible está desplazada por un bias $b$ o $c$. Las unidades visibles representan data observable mientras que las unidades escondidas describen las dependencias entre las variables observadas.

\begin{figure}[htb]
\centering
\includegraphics[width=100mm]{./graficos/rbm.png}
\caption{Arquitectura de la RBM} \label{fig:rbm}
\end{figure}

Para hallar las probabilidades se hace uso de una función de energía:

\begin{equation}
E(x, h) = -\sum_j \sum_k w_{j,k} h_j x_k - \sum_k c_k x_k - \sum_j b_j h_j
\end{equation}

El entrenamiento de esta red consiste básicamente en maximizar la probabilidad del valor de entrada. Para hacer esto se minimiza el promedio de la probabilidad logarítmica usando la gradiente descendente.


\subsection{Deep Belief Network}

En \cite{hinton2006reducing} se demostró que las \ac{RBM} podían ser apiladas y entrenadas de forma voraz para formar las llamadas Deep Belief Networks \cite{erhan2010does}. Las \ac{DBN} son modelos que aprenden a extraer una representación jerárquica profunda de los datos de entrenamiento. Modelan la distribución conjunta de un vector observado $x$ y las $l$ capas ocultas $h^k$ como sigue:

\begin{equation}
P(x, h^1, ..., h^k) = \left(\prod_{k=0}^{l-2} P(h^k|h^{k+1})\right) P(h^{l-1}, h^l)
\end{equation}

Donde $x = h^0$, $P(h^{k-1},h^k)$ es una distribución condicional para las unidades visibles condicionadas por las unidades ocultas de la \ac{RBM} en el nivel $k$, y $P(h^{l-1},h^l)$ es la distribución conjunta de las capas visible e invisible en el nivel más alto de la \ac{RBM}. Esto se ilustra en la figura \ref{fig:rbm_stack}

\begin{figure}[htb]
\centering
\includegraphics[width=40mm]{./graficos/rbm_stack.png}
\caption{Pila de RBM} \label{fig:rbm_stack}
\end{figure}

El principio de entrenamiento no supervisado goloso a nivel de capas puede ser aplicado a las \ac{DBN} con las \ac{RBM} como el bloque de construcción de cada capa \cite{hinton2006reducing, bengio2007greedy}. El proceso es como sigue:

\begin{enumerate}
\item Entrenar la primera capa como una \ac{RBM} que modela la entrada $x=h^0$ como su capa visible
\item Usar esta primera capa para obtener una representación de la entrada que será utilizada como data para la segunda capa. Existen dos soluciones comunes, escoger la representación como las activaciones promedio $p(h^1=1|h^0)$ o como muestras de $p(h^1|h^0)$
\item Entrenar la segunda capa como una \ac{RBM}, tomando la data transformada del paso anterior como ejemplos de entrenamiento para la capa visible de esa \ac{RBM}
\item Iterar (2 y 3) para el número deseado de capas, propagando haci arriba las muestras o activaciones promedio en cada paso
\item Ajuste fino de todos los parámetros de esta arquitectura profunda usando un criterio de aprendizaje supervisado. Por ejemplo, usando la gradiente descendente.
\end{enumerate}

\section{Consideraciones Finales}

En el siguiente capítulo veremos cómo estas ideas pueden ser usadas juntas para potenciarse mutuamente y crear un algoritmo de aprendizaje por refuerzo multiagente. Y a pesar de que surgen ciertas inconsistencias, estas son superadas con la introducción de nuevas estrategias que veremos a continuación.
